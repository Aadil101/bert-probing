# Probing BERT Fine-Tuned For Cultural Background Prediction

## Introduction

## Setup
1. Download Miniconda and create an environment with python 3.8.
2. pip install -r requirements.txt
3. Replace transformer source code with ones provided in bert_src. 
4. Download Cultural Background Datasets (CBD).

## Implementation

## Comparison & Analysis of Results

WandB Diagrams:
- (1x) Table of fine-tuning runs by seed.
- (1x) Training loss curves for BERT fine-tuned on coarse-grained CBD and BERT fine-tuned on finer-grained CBD.
- (1x) Table of some downstream performances.
- (3x) Comparison of downstream performances for BERT (base-cased), BERT fine-tuned on coarse-grained CBD, and BERT fine-tuned on finer-grained CBD. We will show downstream performances for models further fine-tuned on CoLA, NER, and PAWS.

Spreadsheet

## Visualization of Attention

## References
