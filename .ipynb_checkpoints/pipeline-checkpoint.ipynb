{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6006e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pretrained_models import *\n",
    "\n",
    "from experiments.exp_def import TaskDefs\n",
    "from data_utils.log_wrapper import create_logger\n",
    "from data_utils.task_def import EncoderModelType\n",
    "from data_utils.utils import set_environment\n",
    "\n",
    "from mt_dnn.model import MTDNNModel\n",
    "from mt_dnn.batcher import (\n",
    "    SingleTaskDataset,\n",
    "    MultiTaskDataset,\n",
    "    Collater,\n",
    "    MultiTaskBatchSampler\n",
    ")\n",
    "from train_utils import (\n",
    "    dump_opt,\n",
    "    print_message,\n",
    "    save_checkpoint\n",
    ")\n",
    "from gradient_probing import prediction_gradient\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc67e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(parser):\n",
    "    # SET THESE\n",
    "    ##############\n",
    "    parser.add_argument('--multi_gpu_on', action='store_true')\n",
    "    parser.add_argument('--devices', nargs='+')\n",
    "\n",
    "    # head probing\n",
    "    parser.add_argument('--head_probe', action='store_true')\n",
    "    parser.add_argument('--head_probe_layer', type=int)\n",
    "    parser.add_argument('--head_probe_idx', type=int)\n",
    "    parser.add_argument('--head_probe_n_classes', type=int)\n",
    "\n",
    "    # model probing\n",
    "    parser.add_argument('--model_probe', action='store_true')\n",
    "    parser.add_argument('--model_probe_n_classes', type=int)\n",
    "\n",
    "    # gradient probing\n",
    "    parser.add_argument('--gradient_probe', action='store_true')\n",
    "\n",
    "    # finetuning after joint modeling\n",
    "    parser.add_argument('--jm_finetune', action='store_true')\n",
    "    parser.add_argument('--jm_finetune_new', action='store_true') # if task does not exist in 1st stage finetuning\n",
    "    parser.add_argument('--jm_task_id', type=int, default=0) # if task exists from 1st stage finetuning\n",
    "    ##############\n",
    "\n",
    "    # DON\"T NEED THESE\n",
    "    ##############\n",
    "    parser.add_argument('--update_bert_opt', default=0, type=int)\n",
    "    parser.add_argument('--mem_cum_type', type=str, default='simple',\n",
    "                        help='bilinear/simple/defualt')\n",
    "    parser.add_argument('--answer_num_turn', type=int, default=5)\n",
    "    parser.add_argument('--answer_mem_drop_p', type=float, default=0.1)\n",
    "    parser.add_argument('--answer_att_hidden_size', type=int, default=128)\n",
    "    parser.add_argument('--answer_att_type', type=str, default='bilinear',\n",
    "                        help='bilinear/simple/defualt')\n",
    "    parser.add_argument('--answer_rnn_type', type=str, default='gru',\n",
    "                        help='rnn/gru/lstm')\n",
    "    parser.add_argument('--answer_sum_att_type', type=str, default='bilinear',\n",
    "                        help='bilinear/simple/defualt')\n",
    "    parser.add_argument('--answer_merge_opt', type=int, default=1)\n",
    "    parser.add_argument('--answer_mem_type', type=int, default=1)\n",
    "    parser.add_argument('--max_answer_len', type=int, default=10)\n",
    "    parser.add_argument('--answer_dropout_p', type=float, default=0.1)\n",
    "    parser.add_argument('--answer_weight_norm_on', action='store_true')\n",
    "    parser.add_argument('--dump_state_on', action='store_true')\n",
    "    parser.add_argument('--answer_opt', type=int, default=1, help='0,1')\n",
    "    parser.add_argument('--pooler_actf', type=str, default='tanh',\n",
    "                        help='tanh/relu/gelu')\n",
    "    parser.add_argument('--mtl_opt', type=int, default=0)\n",
    "    parser.add_argument('--ratio', type=float, default=0)\n",
    "    parser.add_argument('--mix_opt', type=int, default=0)\n",
    "    parser.add_argument('--max_seq_len', type=int, default=512)\n",
    "    parser.add_argument('--init_ratio', type=float, default=1)\n",
    "    parser.add_argument('--encoder_type', type=int, default=EncoderModelType.BERT)\n",
    "    parser.add_argument('--num_hidden_layers', type=int, default=-1)\n",
    "\n",
    "    # BERT pre-training\n",
    "    parser.add_argument('--bert_model_type', type=str, default='bert-base-multilingual-cased')\n",
    "    parser.add_argument('--init_checkpoint', type=str, default='bert-base-multilingual-cased')\n",
    "    parser.add_argument('--do_lower_case', action='store_true')\n",
    "    parser.add_argument('--masked_lm_prob', type=float, default=0.15)\n",
    "    parser.add_argument('--short_seq_prob', type=float, default=0.2)\n",
    "    parser.add_argument('--max_predictions_per_seq', type=int, default=128)\n",
    "\n",
    "    # bin samples\n",
    "    parser.add_argument('--bin_on', action='store_true')\n",
    "    parser.add_argument('--bin_size', type=int, default=64)\n",
    "    parser.add_argument('--bin_grow_ratio', type=int, default=0.5)\n",
    "\n",
    "    # dist training\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    parser.add_argument(\"--world_size\", type=int, default=1, help=\"For distributed training: world size\")\n",
    "    parser.add_argument(\"--master_addr\", type=str, default=\"localhost\")\n",
    "    parser.add_argument(\"--master_port\", type=str, default=\"6600\")\n",
    "    parser.add_argument(\"--backend\", type=str, default=\"nccl\")\n",
    "    ##############\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def data_config(parser):\n",
    "    # SET THESE\n",
    "    ############\n",
    "    parser.add_argument('--exp_name', default='', help='experiment name')\n",
    "    parser.add_argument('--dataset_name', default='', help='dataset name')\n",
    "    parser.add_argument('--wandb', action='store_true')\n",
    "    ############\n",
    "\n",
    "    # DON'T NEED THESE\n",
    "    ############\n",
    "    parser.add_argument('--log_file', default='mt-dnn.log', help='path for log file.')\n",
    "    parser.add_argument('--data_sort_on', action='store_true')\n",
    "    parser.add_argument('--mkd-opt', type=int, default=0, \n",
    "                        help=\">0 to turn on knowledge distillation, requires 'softlabel' column in input data\")\n",
    "    parser.add_argument('--do_padding', action='store_true')\n",
    "    ############\n",
    "    return parser\n",
    "\n",
    "\n",
    "def train_config(parser):\n",
    "    # SET THESE\n",
    "    ############\n",
    "    parser.add_argument('--epochs', type=int, default=6)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "\n",
    "    # loading\n",
    "    parser.add_argument(\"--model_ckpt\", default='', type=str)\n",
    "    parser.add_argument(\"--resume\", action='store_true')\n",
    "    parser.add_argument('--huggingface_ckpt', action='store_true')\n",
    "    ############\n",
    "\n",
    "    # DON'T NEED THESE\n",
    "    ############\n",
    "    parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available(),\n",
    "                        help='whether to use GPU acceleration.')\n",
    "    parser.add_argument('--log_per_updates', type=int, default=100)\n",
    "    parser.add_argument('--save_per_updates', type=int, default=10000)\n",
    "    parser.add_argument('--save_per_updates_on', action='store_true')\n",
    "    parser.add_argument('--optimizer', default='adamax',\n",
    "                        help='supported optimizer: adamax, sgd, adadelta, adam')\n",
    "    parser.add_argument('--grad_clipping', type=float, default=0)\n",
    "    parser.add_argument('--global_grad_clipping', type=float, default=1.0)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0)\n",
    "    parser.add_argument('--learning_rate', type=float, default=5e-5)\n",
    "    parser.add_argument('--momentum', type=float, default=0)\n",
    "    parser.add_argument('--warmup', type=float, default=0.1)\n",
    "    parser.add_argument('--warmup_schedule', type=str, default='warmup_linear')\n",
    "    parser.add_argument('--adam_eps', type=float, default=1e-6)\n",
    "\n",
    "    parser.add_argument('--vb_dropout', action='store_false')\n",
    "    parser.add_argument('--dropout_p', type=float, default=0.1)\n",
    "    parser.add_argument('--dropout_w', type=float, default=0.000)\n",
    "    parser.add_argument('--bert_dropout_p', type=float, default=0.1)\n",
    "\n",
    "    # scheduler\n",
    "    parser.add_argument('--have_lr_scheduler', dest='have_lr_scheduler', action='store_false')\n",
    "    parser.add_argument('--multi_step_lr', type=str, default='10,20,30')\n",
    "    parser.add_argument('--lr_gamma', type=float, default=0.5)\n",
    "    parser.add_argument('--scheduler_type', type=str, default='ms', help='ms/rop/exp')\n",
    "    parser.add_argument('--output_dir', default='checkpoint')\n",
    "    parser.add_argument('--seed', type=int, default=2018,\n",
    "                        help='random seed for data shuffling, embedding init, etc.')\n",
    "    parser.add_argument('--grad_accumulation_step', type=int, default=1)\n",
    "\n",
    "    #fp 16\n",
    "    parser.add_argument('--fp16', action='store_true',\n",
    "                        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
    "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "\n",
    "    # adv training\n",
    "    parser.add_argument('--adv_train', action='store_true')\n",
    "\n",
    "    # the current release only includes smart perturbation\n",
    "    parser.add_argument('--adv_opt', default=0, type=int)\n",
    "    parser.add_argument('--adv_norm_level', default=0, type=int)\n",
    "    parser.add_argument('--adv_p_norm', default='inf', type=str)\n",
    "    parser.add_argument('--adv_alpha', default=1, type=float)\n",
    "    parser.add_argument('--adv_k', default=1, type=int)\n",
    "    parser.add_argument('--adv_step_size', default=1e-5, type=float)\n",
    "    parser.add_argument('--adv_noise_var', default=1e-5, type=float)\n",
    "    parser.add_argument('--adv_epsilon', default=1e-6, type=float)\n",
    "    parser.add_argument('--encode_mode', action='store_true', help=\"only encode test data\")\n",
    "    parser.add_argument('--debug', action='store_true', help=\"print debug info\")\n",
    "\n",
    "    # transformer cache\n",
    "    parser.add_argument(\"--transformer_cache\", default='.cache', type=str)\n",
    "    ############\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f7a5efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                              '--head_probe', '--head_probe_layer', '11', '--head_probe_idx', '0',\\n                              '--head_probe_n_classes', '13',\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = data_config(parser)\n",
    "parser = model_config(parser)\n",
    "parser = train_config(parser)\n",
    "args = parser.parse_args(args=['--devices', '0', '1', '--multi_gpu_on',\n",
    "                              '--model_probe', '--model_probe_n_classes', '2',\n",
    "                              '--exp_name', 'my_pawsx_exp_2',\n",
    "                              '--dataset_name', 'PAWSX/multi',\n",
    "                              '--wandb',\n",
    "                               '--epochs', '2', '--batch_size', '64',# '--learning_rate', '5e-3',\n",
    "                               '--fp16',\n",
    "                               '--model_ckpt', 'checkpoint/my_pawsx_exp/model_0_3088.pt',\n",
    "                              ])\n",
    "'''\n",
    "                              '--head_probe', '--head_probe_layer', '11', '--head_probe_idx', '0',\n",
    "                              '--head_probe_n_classes', '13',\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05bbef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some stuff in data can be automated\n",
    "dataset_name = args.dataset_name\n",
    "args.data_dir = f'experiments/{dataset_name}/{args.bert_model_type}'\n",
    "args.task_def = f'experiments/{dataset_name}/task_def.yaml'\n",
    "if \"/\" in dataset_name:\n",
    "    args.train_datasets = dataset_name.split(\"/\")[0].lower()\n",
    "else:\n",
    "    args.train_datasets = dataset_name.lower()\n",
    "\n",
    "# set task name, root data dir, and output dir.\n",
    "output_dir = args.output_dir\n",
    "data_dir = args.data_dir\n",
    "\n",
    "# multiple datasets are split by '_'\n",
    "args.train_datasets = args.train_datasets.split('_')\n",
    "\n",
    "# seed everything.\n",
    "set_environment(args.seed, args.cuda)\n",
    "\n",
    "# stores task: param_args for each TaskDef param\n",
    "task_defs = TaskDefs(args.task_def)\n",
    "encoder_type = args.encoder_type\n",
    "\n",
    "exp_name = args.exp_name\n",
    "\n",
    "# make log dir and set logger.\n",
    "log_path = f'logs/{exp_name}/{args.log_file}'\n",
    "Path(log_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "logger = create_logger(__name__, to_disk=True, log_file=log_path)\n",
    "\n",
    "# make output dir and set to absolute path.\n",
    "if not args.head_probe:\n",
    "    output_dir = Path(output_dir).joinpath(exp_name)\n",
    "\n",
    "output_dir = Path(os.path.abspath(output_dir))\n",
    "output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e73f42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/14/2022 12:55:22 Launching MT-DNN training.\n"
     ]
    }
   ],
   "source": [
    "print_message(logger, 'Launching MT-DNN training.')\n",
    "opt = vars(args)\n",
    "args.devices = [int(g) for g in args.devices]\n",
    "\n",
    "tasks = {}\n",
    "task_def_list = []\n",
    "train_data_lists = []\n",
    "train_datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b6a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/14/2022 12:55:22 Loading experiments/PAWSX/multi/bert-base-multilingual-cased/pawsx_train.json as task 0\n",
      "Loaded 197604 samples out of 197604\n"
     ]
    }
   ],
   "source": [
    "# create training dataset.\n",
    "for dataset in args.train_datasets:\n",
    "    prefix = dataset.split('_')[0]\n",
    "    if prefix not in tasks:\n",
    "        task_id = len(tasks)\n",
    "        tasks[prefix] = task_id\n",
    "\n",
    "        task_def = task_defs.get_task_def(prefix)\n",
    "        task_def_list.append(task_def)\n",
    "\n",
    "        train_path = os.path.join(data_dir, '{}_train.json'.format(dataset))\n",
    "        print_message(logger, 'Loading {} as task {}'.format(train_path, task_id))\n",
    "        \n",
    "        train_data_set = SingleTaskDataset(\n",
    "            path=train_path,\n",
    "            is_train=True,\n",
    "            maxlen=args.max_seq_len,\n",
    "            task_id=task_id,\n",
    "            task_def=task_def,\n",
    "            printable=True)\n",
    "\n",
    "        train_datasets.append(train_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb96df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collater = Collater(\n",
    "    dropout_w=args.dropout_w,\n",
    "    encoder_type=encoder_type,\n",
    "    soft_label=False,\n",
    "    max_seq_len=args.max_seq_len,\n",
    "    do_padding=args.do_padding)\n",
    "\n",
    "multi_task_train_dataset = MultiTaskDataset(train_datasets)\n",
    "multi_task_batch_sampler = MultiTaskBatchSampler(\n",
    "                            train_datasets,\n",
    "                            args.batch_size,\n",
    "                            args.mix_opt,\n",
    "                            args.ratio,\n",
    "                            bin_on=args.bin_on,\n",
    "                            bin_size=args.bin_size,\n",
    "                            bin_grow_ratio=args.bin_grow_ratio)\n",
    "\n",
    "multi_task_train_dataloader = DataLoader(\n",
    "    multi_task_train_dataset,\n",
    "    batch_sampler=multi_task_batch_sampler,\n",
    "    collate_fn=train_collater.collate_fn,\n",
    "    pin_memory=len(args.devices)>0)\n",
    "\n",
    "train_data_lists.append(multi_task_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663563a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/14/2022 12:55:27 ############# Gradient Accumulation Info #############\n",
      "04/14/2022 12:55:27 number of step: 6176\n",
      "04/14/2022 12:55:27 number of grad grad_accumulation step: 1\n",
      "04/14/2022 12:55:27 adjusted number of step: 6176\n",
      "04/14/2022 12:55:27 #######################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# div number of grad accumulation. \n",
    "n_batch_per_epoch = len(multi_task_train_dataloader) // args.grad_accumulation_step\n",
    "num_all_batches = args.epochs * n_batch_per_epoch\n",
    "print_message(logger, '############# Gradient Accumulation Info #############')\n",
    "print_message(logger, 'number of step: {}'.format(args.epochs * len(multi_task_train_dataloader)))\n",
    "print_message(logger, 'number of grad grad_accumulation step: {}'.format(args.grad_accumulation_step))\n",
    "print_message(logger, 'adjusted number of step: {}'.format(num_all_batches))\n",
    "print_message(logger, '#######################################\\n')\n",
    "\n",
    "if opt['encoder_type'] not in EncoderModelType._value2member_map_:\n",
    "    raise ValueError(\"encoder_type is out of pre-defined types\")\n",
    "\n",
    "literal_encoder_type = EncoderModelType(opt['encoder_type']).name.lower()\n",
    "config_class, _, _ = MODEL_CLASSES[literal_encoder_type]\n",
    "config = config_class.from_pretrained(args.bert_model_type).to_dict()\n",
    "\n",
    "config['attention_probs_dropout_prob'] = args.bert_dropout_p\n",
    "config['hidden_dropout_prob'] = args.bert_dropout_p\n",
    "config['multi_gpu_on'] = opt[\"multi_gpu_on\"]\n",
    "\n",
    "if args.num_hidden_layers > 0:\n",
    "    config['num_hidden_layers'] = args.num_hidden_layers\n",
    "\n",
    "opt['task_def_list'] = task_def_list\n",
    "opt['head_probe'] = args.head_probe\n",
    "opt.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cc47744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if resuming, load state dict, and get init epoch and step.\n",
    "if args.resume:\n",
    "    assert args.model_ckpt != '' and Path(args.model_ckpt).is_file(), args.model_ckpt\n",
    "    print_message(logger, f'loading model from {args.model_ckpt}')           \n",
    "    state_dict = torch.load(args.model_ckpt, map_location=f'cuda:{args.devices[0]}')\n",
    "\n",
    "    if args.jm_finetune:\n",
    "        if args.jm_finetune_new:\n",
    "            i = 0\n",
    "            while f'scoring_list.{i}.weight' in state_dict['state']:\n",
    "                del state_dict['state'][f'scoring_list.{i}.weight']\n",
    "                del state_dict['state'][f'scoring_list.{i}.bias']\n",
    "                i += 1\n",
    "        else:\n",
    "            i = 0\n",
    "            while f'scoring_list.{i}.weight' in state_dict['state']:\n",
    "                if i != args.jm_task_id:\n",
    "                    del state_dict['state'][f'scoring_list.{i}.weight']\n",
    "                    del state_dict['state'][f'scoring_list.{i}.bias']\n",
    "                i += 1\n",
    "\n",
    "            if args.jm_task_id != 0:\n",
    "                for param in ['weight', 'bias']:\n",
    "                    state_dict['state'][f'scoring_list.0.{param}'] = state_dict['state'][f'scoring_list.{args.jm_task_id}.{param}']\n",
    "                    del state_dict['state'][f'scoring_list.{args.jm_task_id}.{param}']\n",
    "\n",
    "    if not args.huggingface_ckpt:\n",
    "        split_model_name = args.model_ckpt.split(\"/\")[-1].split(\"_\")\n",
    "        if len(split_model_name) > 2:\n",
    "            init_epoch_idx = int(split_model_name[1]) + 1\n",
    "            init_global_step = int(split_model_name[2].split(\".\")[0]) + 1\n",
    "        else:\n",
    "            init_epoch_idx = int(split_model_name[1].split(\".\")[0]) + 1\n",
    "            init_global_step = 0\n",
    "    else:\n",
    "        init_epoch_idx = 0\n",
    "        init_global_step = 0\n",
    "\n",
    "else:\n",
    "    state_dict = None\n",
    "    init_epoch_idx = 0\n",
    "    init_global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3c33bf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "model = MTDNNModel(\n",
    "        opt,\n",
    "        devices=args.devices,\n",
    "        num_train_step=num_all_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76602cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (args.head_probe or args.model_probe):\n",
    "    if state_dict is not None and args.resume:\n",
    "        if args.huggingface_ckpt:\n",
    "            if args.bert_model_type == 'bert-base-multilingual-cased':\n",
    "                params_to_remove = [\n",
    "                    \"cls.predictions.bias\",\n",
    "                    \"cls.predictions.transform.dense.weight\",\n",
    "                    \"cls.predictions.transform.dense.bias\",\n",
    "                    \"cls.predictions.transform.LayerNorm.weight\",\n",
    "                    \"cls.predictions.transform.LayerNorm.bias\",\n",
    "                    \"cls.predictions.decoder.weight\",\n",
    "                    \"cls.predictions.decoder.bias\"\n",
    "                ]\n",
    "            elif args.bert_model_type == 'xlm-roberta-base':\n",
    "                params_to_remove = []\n",
    "\n",
    "                # rename roberta -> bert\n",
    "                renamed_state_dict = {}\n",
    "                for n, p in state_dict.items():\n",
    "                    new_name = n.split('.')\n",
    "                    if new_name[0] == 'roberta':\n",
    "                        new_name[0] = 'bert'\n",
    "                        new_name = '.'.join(new_name)\n",
    "                        renamed_state_dict[new_name] = p\n",
    "                state_dict = renamed_state_dict\n",
    "\n",
    "            for param_name in params_to_remove:\n",
    "                if param_name in state_dict:\n",
    "                    print(f'{param_name} in state_dict, removing')\n",
    "                    del state_dict[param_name]\n",
    "                else:\n",
    "                    print(f'{param_name} not in state_dict')\n",
    "\n",
    "            _init_state_dict = model.network.state_dict()\n",
    "\n",
    "            params_to_add = [\n",
    "                    \"bert.pooler.dense.weight\",\n",
    "                    \"bert.pooler.dense.bias\",\n",
    "                    \"pooler.dense.weight\",\n",
    "                    \"pooler.dense.bias\"\n",
    "                ]\n",
    "            num_tasks = len(task_def_list)\n",
    "            for i in range(num_tasks):\n",
    "                params_to_add.extend([f\"scoring_list.{i}.weight\", f\"scoring_list.{i}.bias\"])\n",
    "\n",
    "            for param_name in params_to_add:\n",
    "                state_dict[param_name] = _init_state_dict[param_name]\n",
    "\n",
    "            state_dict = {'state': state_dict}\n",
    "\n",
    "        if args.jm_finetune and args.jm_finetune_new:\n",
    "            for param in ['weight', 'bias']:\n",
    "                state_dict['state'][f'scoring_list.0.{param}'] = model.network.state_dict()[f'scoring_list.0.{param}']\n",
    "\n",
    "        model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8350401",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.head_probe:\n",
    "    print_message(logger, f'attached head probe at layer #{args.head_probe_layer+1}, head #{args.head_probe_idx+1}')\n",
    "    opt['head_idx_to_probe'] = (args.head_probe_layer, args.head_probe_idx)\n",
    "\n",
    "    # freeze all params\n",
    "    for p in model.network.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # load model, making sure to match scoring_list params\n",
    "    if args.model_ckpt != '':\n",
    "        state_dict = torch.load(args.model_ckpt, map_location=f'cuda:{args.devices[0]}')\n",
    "        state_dict['state']['scoring_list.0.weight'] = model.network.state_dict()['scoring_list.0.weight']\n",
    "        state_dict['state']['scoring_list.0.bias'] = model.network.state_dict()['scoring_list.0.bias']\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    # then attach probing head\n",
    "    model.attach_head_probe(\n",
    "        args.head_probe_layer,\n",
    "        args.head_probe_idx,\n",
    "        n_classes=args.head_probe_n_classes)\n",
    "    optimizer_parameters = model._get_param_groups()\n",
    "    model._setup_optim(optimizer_parameters, None, num_all_batches)\n",
    "\n",
    "    init_epoch_idx = 0\n",
    "    init_global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7f35d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/14/2022 12:55:31 probing whole model. Setting n_epochs to 2.\n",
      "model_probe, loading model ckpt from checkpoint/my_pawsx_exp/model_0_3088.pt\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "if args.model_probe:\n",
    "    print_message(logger, 'probing whole model. Setting n_epochs to 2.')\n",
    "    args.epochs = 2\n",
    "\n",
    "    # freeze all params\n",
    "    for p in model.network.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # load model, making sure to match scoring_list params\n",
    "    if args.model_ckpt != '':\n",
    "        print(f'model_probe, loading model ckpt from {args.model_ckpt}')\n",
    "        state_dict = torch.load(args.model_ckpt, map_location=f'cuda:{args.devices[0]}')\n",
    "        state_dict['state']['scoring_list.0.weight'] = model.network.state_dict()['scoring_list.0.weight']\n",
    "        state_dict['state']['scoring_list.0.bias'] = model.network.state_dict()['scoring_list.0.bias']\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    # then attach probing head\n",
    "    model.attach_model_probe(n_classes=args.model_probe_n_classes)\n",
    "    optimizer_parameters = model._get_param_groups()\n",
    "    model._setup_optim(optimizer_parameters, None, num_all_batches)\n",
    "\n",
    "    init_epoch_idx = 0\n",
    "    init_global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "083541a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maadilislam\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.14 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aadil/bert-probing/wandb/run-20220414_125532-21nsbtxb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aadilislam/soroush/runs/21nsbtxb\" target=\"_blank\">my_pawsx_exp_2</a></strong> to <a href=\"https://wandb.ai/aadilislam/soroush\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dump config\n",
    "dump_opt(opt, output_dir)\n",
    "\n",
    "if args.gradient_probe:\n",
    "    save_path = Path('gradient_probe_outputs').joinpath(exp_name)\n",
    "    prediction_gradient(\n",
    "        args,\n",
    "        model, \n",
    "        multi_task_train_dataloader,\n",
    "        save_path\n",
    "    )\n",
    "    #return\n",
    "\n",
    "if args.wandb:\n",
    "    wandb.init(project='soroush', name=exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7537c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/14/2022 12:55:37 At epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadil/anaconda3/envs/venv3.8/lib/python3.8/site-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/14/2022 12:55:38 [e0] [1/3088] train loss: 1.11672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadil/anaconda3/envs/venv3.8/lib/python3.8/site-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  return orig_fn(arg0, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/14/2022 12:56:01 [e0] [101/3088] train loss: 1.10718\n",
      "04/14/2022 12:56:23 [e0] [201/3088] train loss: 0.95459\n",
      "04/14/2022 12:56:46 [e0] [301/3088] train loss: 0.76995\n",
      "04/14/2022 12:57:09 [e0] [401/3088] train loss: 0.62720\n",
      "04/14/2022 12:57:31 [e0] [501/3088] train loss: 0.53221\n",
      "04/14/2022 12:57:54 [e0] [601/3088] train loss: 0.46638\n",
      "04/14/2022 12:58:16 [e0] [701/3088] train loss: 0.41938\n",
      "04/14/2022 12:58:39 [e0] [801/3088] train loss: 0.38159\n",
      "04/14/2022 12:59:01 [e0] [901/3088] train loss: 0.35141\n",
      "04/14/2022 12:59:24 [e0] [1001/3088] train loss: 0.32761\n",
      "04/14/2022 12:59:47 [e0] [1101/3088] train loss: 0.30788\n",
      "04/14/2022 01:00:09 [e0] [1201/3088] train loss: 0.29102\n",
      "04/14/2022 01:00:32 [e0] [1301/3088] train loss: 0.27571\n",
      "04/14/2022 01:00:55 [e0] [1401/3088] train loss: 0.26313\n",
      "04/14/2022 01:01:17 [e0] [1501/3088] train loss: 0.25172\n",
      "04/14/2022 01:01:39 [e0] [1601/3088] train loss: 0.24248\n",
      "04/14/2022 01:02:03 [e0] [1701/3088] train loss: 0.23433\n",
      "04/14/2022 01:02:25 [e0] [1801/3088] train loss: 0.22681\n",
      "04/14/2022 01:02:47 [e0] [1901/3088] train loss: 0.21997\n",
      "04/14/2022 01:03:10 [e0] [2001/3088] train loss: 0.21422\n",
      "04/14/2022 01:03:33 [e0] [2101/3088] train loss: 0.20938\n",
      "04/14/2022 01:03:55 [e0] [2201/3088] train loss: 0.20483\n",
      "04/14/2022 01:04:18 [e0] [2301/3088] train loss: 0.20129\n",
      "04/14/2022 01:04:41 [e0] [2401/3088] train loss: 0.19739\n",
      "04/14/2022 01:05:03 [e0] [2501/3088] train loss: 0.19447\n",
      "04/14/2022 01:05:26 [e0] [2601/3088] train loss: 0.19196\n",
      "04/14/2022 01:05:48 [e0] [2701/3088] train loss: 0.18963\n",
      "04/14/2022 01:06:11 [e0] [2801/3088] train loss: 0.18841\n",
      "04/14/2022 01:06:34 [e0] [2901/3088] train loss: 0.18716\n",
      "04/14/2022 01:06:56 [e0] [3001/3088] train loss: 0.18660\n",
      "04/14/2022 01:07:17 Saving mt-dnn model to /home/aadil/bert-probing/checkpoint/my_pawsx_exp_1/model_0_3088.pt\n",
      "04/14/2022 01:07:17 At epoch 1\n",
      "04/14/2022 01:07:19 [e1] [13/3088] train loss: 0.18666\n",
      "04/14/2022 01:07:42 [e1] [113/3088] train loss: 0.18748\n",
      "04/14/2022 01:08:05 [e1] [213/3088] train loss: 0.18769\n",
      "04/14/2022 01:08:27 [e1] [313/3088] train loss: 0.18725\n",
      "04/14/2022 01:08:50 [e1] [413/3088] train loss: 0.18685\n",
      "04/14/2022 01:09:13 [e1] [513/3088] train loss: 0.18660\n",
      "04/14/2022 01:09:35 [e1] [613/3088] train loss: 0.18649\n",
      "04/14/2022 01:09:58 [e1] [713/3088] train loss: 0.18649\n",
      "04/14/2022 01:10:21 [e1] [813/3088] train loss: 0.18604\n",
      "04/14/2022 01:10:43 [e1] [913/3088] train loss: 0.18564\n",
      "04/14/2022 01:11:06 [e1] [1013/3088] train loss: 0.18524\n",
      "04/14/2022 01:11:28 [e1] [1113/3088] train loss: 0.18476\n",
      "04/14/2022 01:11:51 [e1] [1213/3088] train loss: 0.18433\n",
      "04/14/2022 01:12:14 [e1] [1313/3088] train loss: 0.18393\n",
      "04/14/2022 01:12:36 [e1] [1413/3088] train loss: 0.18352\n",
      "04/14/2022 01:12:59 [e1] [1513/3088] train loss: 0.18319\n",
      "04/14/2022 01:13:22 [e1] [1613/3088] train loss: 0.18356\n",
      "04/14/2022 01:13:44 [e1] [1713/3088] train loss: 0.18340\n",
      "04/14/2022 01:14:07 [e1] [1813/3088] train loss: 0.18378\n",
      "04/14/2022 01:14:30 [e1] [1913/3088] train loss: 0.18422\n",
      "04/14/2022 01:14:52 [e1] [2013/3088] train loss: 0.18536\n",
      "04/14/2022 01:15:15 [e1] [2113/3088] train loss: 0.18713\n",
      "04/14/2022 01:15:37 [e1] [2213/3088] train loss: 0.18907\n",
      "04/14/2022 01:16:00 [e1] [2313/3088] train loss: 0.19155\n",
      "04/14/2022 01:16:22 [e1] [2413/3088] train loss: 0.19409\n",
      "04/14/2022 01:16:45 [e1] [2513/3088] train loss: 0.19798\n",
      "04/14/2022 01:17:08 [e1] [2613/3088] train loss: 0.20282\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n",
      "04/14/2022 01:17:30 [e1] [2713/3088] train loss: 0.20837\n",
      "04/14/2022 01:17:53 [e1] [2813/3088] train loss: 0.21637\n",
      "04/14/2022 01:18:15 [e1] [2913/3088] train loss: 0.22599\n",
      "04/14/2022 01:18:38 [e1] [3013/3088] train loss: 0.23705\n",
      "04/14/2022 01:18:56 Saving mt-dnn model to /home/aadil/bert-probing/checkpoint/my_pawsx_exp_1/model_1_6176.pt\n"
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "for epoch in range(init_epoch_idx, init_epoch_idx+args.epochs):        \n",
    "    print_message(logger, f'At epoch {epoch}', level=1)\n",
    "\n",
    "    for (batch_meta, batch_data) in multi_task_train_dataloader:\n",
    "        batch_meta, batch_data = Collater.patch_data(\n",
    "            torch.device(args.devices[0]),\n",
    "            batch_meta,\n",
    "            batch_data)\n",
    "\n",
    "        task_id = batch_meta['task_id']\n",
    "        model.update(batch_meta, batch_data)\n",
    "\n",
    "        if (model.updates - 1) % (args.log_per_updates) == 0:\n",
    "            print_message(logger, f\"[e{epoch}] [{model.updates % n_batch_per_epoch}/{n_batch_per_epoch}] train loss: {model.train_loss.avg:.5f}\")\n",
    "\n",
    "            if args.wandb:\n",
    "                wandb.log({\n",
    "                    'train/loss': model.train_loss.avg,\n",
    "                    'global_step': init_global_step + model.updates,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "\n",
    "    model_file = save_checkpoint(model, epoch, output_dir)\n",
    "    print_message(logger, f'Saving mt-dnn model to {model_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a3655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size -> speed in seconds/batch\n",
    "# 64 -> 36 (apex broken fp16)\n",
    "# 64 -> 44 (torch.cuda.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37947fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadil/anaconda3/envs/venv3.8/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "  warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from get_model_probe_results import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28720767",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 0\n",
    "finetuned_task = Experiment.PAWSX\n",
    "finetuned_setting = LingualSetting.MULTI\n",
    "probe_setting = LingualSetting.MULTI\n",
    "probe_task = Experiment.PAWSX\n",
    "model_ckpt = 'checkpoint/my_pawsx_exp_1/model_1_6176.pt'\n",
    "out_file_name = 'results.csv'\n",
    "metric = 'ACC'\n",
    "batch_size = 64\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed16cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_probe_outputs/PAWSX_multi\n",
      "data from experiments/PAWSX/multi/bert-base-multilingual-cased/pawsx_test.json\n",
      "Loaded 8000 samples out of 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PAWSX_multi -> PAWSX [multi], multi_head_training\n",
      "loading from checkpoint/my_pawsx_exp_1/model_1_6176.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                              | 0/125 [00:00<?, ?it/s]/home/aadil/anaconda3/envs/venv3.8/lib/python3.8/site-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "100%|████████████████████████████████████| 125/125 [00:24<00:00,  5.19it/s]\n"
     ]
    }
   ],
   "source": [
    "get_model_probe_scores(\n",
    "    finetuned_task,\n",
    "    finetuned_setting,\n",
    "    probe_setting,\n",
    "    probe_task,\n",
    "    model_ckpt,\n",
    "    out_file_name,\n",
    "    metric,\n",
    "    device_id,\n",
    "    'multi',\n",
    "    batch_size,\n",
    "    max_seq_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44b03c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv3.8] *",
   "language": "python",
   "name": "conda-env-venv3.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
